{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T20:48:36.372732Z",
     "start_time": "2023-12-28T20:48:36.353613Z"
    }
   },
   "outputs": [],
   "source": [
    "import functools # https://docs.python.org/3/library/functools.html\n",
    "import glob # short for global, is a function that's used to search for files that match a specific file pattern or name.\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "\n",
    "import SimpleITK as sitk # https://simpleitk.org/ (We can treat the format of the data files as a black box and use SimpleITK to load them into more familiar NumPy arrays.)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from util.util import XyzTuple, xyz2irc\n",
    "from util.disk import getCache\n",
    "from util.logconf import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T20:48:36.375212Z",
     "start_time": "2023-12-28T20:48:36.368790Z"
    }
   },
   "outputs": [],
   "source": [
    "CandidateInfoTuple = namedtuple(\n",
    "    'CandidateInfoTuple',\n",
    "    'isNodule_bool, diameter_mm, series_uid, center_xyz',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getCandidateInfoList comments\n",
    "\n",
    "* *@functools.lru_cache(1) - in-memory cache decorator*\n",
    "* *requireOnDisk_bool=True - defaults to screening out series from data subsets that aren't in place yet.*\n",
    "* *We construct a set with all series_uids that are present on disk. This will let us use the data, even if we haven't downloaded all of the subsets yet.*\n",
    "* *For each of the candidate entries for a given series_uid, we loop through the annotations we collected earlier for the same series_uid and see if the two coordinates are\n",
    "close enough to consider them the same nodule.*\n",
    "* *If we don’t find diameter information for a nodule, that’s fine; we’ll just treat the nodule as having a 0.0 diameter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T20:51:53.407450Z",
     "start_time": "2023-12-28T20:51:53.394786Z"
    }
   },
   "outputs": [],
   "source": [
    "@functools.lru_cache(1)\n",
    "def getCandidateInfoList(requireOnDisk_bool=True):\n",
    "    mhd_list = glob.glob('data/subset*/*.mhd') # Return a possibly empty list of path names that match pathname\n",
    "    present_on_disk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
    "    \n",
    "    print(\"candidates present on disk: \", present_on_disk_set)\n",
    "\n",
    "    # First we need to group our annotations by series_uid, as that’s the first key we’ll use to cross-reference each row from the two files.\n",
    "    diameter_dict = {}\n",
    "\n",
    "    # seriesuid,coordX,coordY,coordZ,diameter_mm - header of 'annotations.csv'\n",
    "    with open('data/annotations.csv', 'r') as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "            annotationDiameter_mm = float(row[4])\n",
    "\n",
    "            diameter_dict.setdefault(series_uid, []).append(\n",
    "                (annotationCenter_xyz, annotationDiameter_mm)\n",
    "            )\n",
    "\n",
    "    # Now we’ll build our full list of candidates using the information in the candidates.csv file.\n",
    "    candidateInfo_list = []\n",
    "\n",
    "    # seriesuid,coordX,coordY,coordZ,class - header of 'candidates.csv'\n",
    "    with open('data/candidates.csv', 'r') as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "\n",
    "            # If a series_uid isn’t present, it’s in a subset we don’t have on disk, so we should skip it.\n",
    "            if series_uid not in present_on_disk_set and requireOnDisk_bool:\n",
    "                continue\n",
    "\n",
    "            isNodule = bool(int(row[4]))\n",
    "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "\n",
    "            candidateDiameter_mm = 0.0\n",
    "            for annotation_tuple in diameter_dict.get(series_uid, []):\n",
    "                annotationCenter_xyz, annotationDiameter_mm = annotation_tuple\n",
    "                for i in range(3):\n",
    "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
    "                    \n",
    "                    # Divides the diameter by 2 to get the radius, and divides the radius by 2 to require that the two nodule center points not be too far apart relative to the size of the nodule. \n",
    "                    # (This results in a bounding-box check, not a true distance check.) \n",
    "                    if delta_mm > annotationDiameter_mm / 4:\n",
    "                        break\n",
    "                    else:\n",
    "                        candidateDiameter_mm = annotationDiameter_mm\n",
    "                        break\n",
    "\n",
    "                candidateInfo_list.append(CandidateInfoTuple(\n",
    "                    isNodule,\n",
    "                    candidateDiameter_mm,\n",
    "                    series_uid,\n",
    "                    candidateCenter_xyz,\n",
    "                ))\n",
    "\n",
    "    # This means we have all the actual nodule samples starting with the largest first, \n",
    "    # followed by all the non-nodule samples (which don’t have nodule size information).\n",
    "    candidateInfo_list.sort(reverse=True)\n",
    "    return candidateInfo_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *The 10 subsets we discussed earlier have about 90 CT scans each (888 in total), with every CT scan represented as two files: one with a .mhd extension and one with a .raw extension. The data being split between multiple files is hidden behind the sitk routines, however, and is not something we need to be directly concerned with.*\n",
    "\n",
    "* *Continuing the __init__ method, we need to do a bit of cleanup on the ct_a val- ues. CT scan voxels are expressed in Hounsfield units (HU; https://en.wikipedia.org/wiki/Hounsfield_scale), which are odd units; air is –1,000 HU (close enough to 0 g/cc [grams per cubic centimeter] for our purposes), water is 0 HU (1 g/cc), and bone is at least +1,000 HU (2–3 g/cc).*\n",
    "\n",
    "* *It’s important to know that our data uses the range of –1,000 to +1,000.*\n",
    "\n",
    "* *Candidate center data is expressed in millimeters, not voxels. We need to transform our coordinates from the millimeter-based coordinate system (X,Y,Z) they’re expressed in, to the voxel-address-based coordinate system (I,R,C) used to take array slices from our CT scan data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![title](images/convertToIRC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with CT scans, we refer to the array dimensions as index, row, and column, because a separate meaning exists for X, Y, and Z, as illustrated in figure 10.6. The patient coordinate system defines positive X to be patient- left (left), positive Y to be patient-behind (posterior), and positive Z to be toward-patient- head (superior). **Left-posterior-superior is sometimes abbreviated LPS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![title](images/figure10_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The patient coordinate system is measured in millimeters and has an arbitrarily positioned origin that does not correspond to the origin of the CT voxel array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![title](images/figure10_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotted using square pixels, the non-cubic voxels can end up looking some- what distorted, similar to the distortion near the north and south poles when using a Mercator projection map. We will need to apply a scaling factor if we want the images to depict realistic proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CTs are commonly 512 rows by 512 columns, with the index dimension ranging from around 100 total slices up to perhaps 250 slices (250 slices times 2.5 millimeters is typically enough to contain the anatomical region of interest). This results in a lower bound of approximately 225 voxels, or about 32 million data points. Each CT specifies the voxel size in millimeters as part of the file metadata;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the math manually:\n",
    "* Flip the coordinates from IRC to CRI, to align with XYZ.\n",
    "* Scale the indices with the voxel sizes.\n",
    "* Matrix-multiply with the directions matrix, using @ in Python.\n",
    "* Add the offset for the origin.\n",
    "\n",
    "To go back from XYZ to IRC, we need to perform the inverse of each step in the reverse order. The metadata we need to convert from patient coordinates (_xyz) to array coordinates (_irc) is contained in the MetaIO file alongside the CT data itself. We pull the voxel sizing and positioning metadata out of the .mhd file at the same time we get the ct_a.\n",
    "\n",
    "* The getRawNodule function takes the center expressed in the patient coordinate sys- tem (X,Y,Z), just as it’s specified in the LUNA CSV data, as well as a width in voxels. It returns a cubic chunk of CT, as well as the center of the candidate converted to array coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T20:48:36.396267Z",
     "start_time": "2023-12-28T20:48:36.393368Z"
    }
   },
   "outputs": [],
   "source": [
    "class CT:\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob(\n",
    "            'data/subset*/{}.mhd'.format(series_uid) # We don’t care to track which subset a given series_uid is in, so we wildcard the subset.\n",
    "        )[0]\n",
    "\n",
    "        # sitk.ReadImage implicitly consumes the .raw file in addition to the passed-in .mhd file.\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "\n",
    "        # Recreates an np.array since we want to convert the value type to np.float32\n",
    "        # ct_a is a three-dimensional array.\n",
    "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
    "\n",
    "        # Capping the values between -1000 and 1000\n",
    "        ct_a.clip(-1000, 1000, ct_a)\n",
    "\n",
    "        self.series_uid = series_uid\n",
    "        self.hu_a = ct_a\n",
    "\n",
    "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
    "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
    "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)  # Converts the directions to an array, and reshapes the nine-element array to its proper 3x3 matrix shape\n",
    "\n",
    "    def getRawCandidate(self, center_xyz, width_irc):\n",
    "        center_irc = xyz2irc(\n",
    "            center_xyz,\n",
    "            self.origin_xyz,\n",
    "            self.vxSize_xyz,\n",
    "            self.direction_a\n",
    "        )\n",
    "\n",
    "        slice_list = []\n",
    "        for axis, center_val in enumerate(center_irc):\n",
    "            start_ndx = int(round(center_val - width_irc[axis] / 2))\n",
    "            end_ndx = int(start_ndx + width_irc[axis])\n",
    "\n",
    "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
    "\n",
    "            if start_ndx < 0:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                start_ndx = 0\n",
    "                end_ndx = int(width_irc[axis])\n",
    "\n",
    "            if end_ndx > self.hu_a.shape[axis]:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                end_ndx = self.hu_a.shape[axis]\n",
    "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
    "\n",
    "            slice_list.append(slice(start_ndx, end_ndx))\n",
    "\n",
    "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
    "\n",
    "        return ct_chunk, center_irc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![title](images/figure10_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By subclassing Dataset, we will take our arbitrary data and plug it into the rest of the PyTorch ecosystem.\n",
    "\n",
    "* Our LunaDataset class will normalize those samples, flattening each CT’s nodules into a single collection from which samples can be retrieved without regard for which Ct instance the sample originates from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch API only requires that any Dataset subclasses we want to implement must provide these two functions:\n",
    "* An implementation of __len__ that must return a single, constant value after initialization (the value ends up being cached in some use cases)\n",
    "* The __getitem__ method, which takes an index and returns a tuple with sample data to be used for training (or validation, as the case may be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T20:48:36.500254Z",
     "start_time": "2023-12-28T20:48:36.405083Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_cache = getCache('part2ch10_raw')\n",
    "\n",
    "@functools.lru_cache(1, typed=True)\n",
    "def getCt(series_uid):\n",
    "    return CT(series_uid)\n",
    "\n",
    "# The implementation of 'raw_cache' in util folder\n",
    "@raw_cache.memoize(typed=True)\n",
    "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
    "    ct = getCt(series_uid)\n",
    "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
    "    return ct_chunk, center_irc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T20:48:36.507682Z",
     "start_time": "2023-12-28T20:48:36.501573Z"
    }
   },
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARN)\n",
    "# log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T20:48:36.514742Z",
     "start_time": "2023-12-28T20:48:36.507530Z"
    }
   },
   "outputs": [],
   "source": [
    "class LunaDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 val_stride=0,\n",
    "                 isValSet_bool=None,\n",
    "                 series_uid=None,\n",
    "                 ):\n",
    "        self.candidateInfo_list = copy.copy(getCandidateInfoList()) # Copies the return value so the cached copy won’t be impacted by altering self.candidateInfo_list\n",
    "\n",
    "        if series_uid:\n",
    "            self.candidateInfo_list = [\n",
    "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
    "            ]\n",
    "\n",
    "        if isValSet_bool:\n",
    "            assert val_stride > 0, val_stride\n",
    "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "        elif val_stride > 0:\n",
    "            del self.candidateInfo_list[::val_stride] # Deletes the validation images (every val_stride-th item in the list) from self.candidateInfo_list. We made a copy earlier so that we don’t alter the original list.\n",
    "            assert self.candidateInfo_list\n",
    "\n",
    "        log.info(\"{!r}: {} {} samples\".format(\n",
    "            self,\n",
    "            len(self.candidateInfo_list),\n",
    "            \"validation\" if isValSet_bool else \"training\",\n",
    "        ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.candidateInfo_list)\n",
    "    \n",
    "    def __getitem__(self, ndx):\n",
    "        candidateInfo_tup = self.candidateInfo_list[ndx]\n",
    "        width_irc = (32, 48, 48)\n",
    "\n",
    "        candidate_a, center_irc = getCtRawCandidate(    # The return value candidate_a has shape (32,48,48), the axes are depth, height, and width\n",
    "            candidateInfo_tup.series_uid,\n",
    "            candidateInfo_tup.center_xyz,\n",
    "            width_irc\n",
    "        )\n",
    "\n",
    "        # The next thing we need to do in the __getitem__ method is manipulate the data into the proper data types and required array dimensions that will be expected by downstream code.\n",
    "        candidate_t = torch.from_numpy(candidate_a)\n",
    "        candidate_t = candidate_t.to(torch.float32)\n",
    "        candidate_t = candidate_t.unsqueeze(0) # .unsqueeze(0) adds the 'Channel' dimension\n",
    "\n",
    "        # This has two elements, one each for our possible candidate classes (nodule or non- nodule; or positive or negative, respectively). \n",
    "        pos_t = torch.tensor([\n",
    "                not candidateInfo_tup.isNodule_bool,\n",
    "                candidateInfo_tup.isNodule_bool\n",
    "            ],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            candidate_t,\n",
    "            pos_t,\n",
    "            candidateInfo_tup.series_uid,\n",
    "            torch.tensor(center_irc)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
