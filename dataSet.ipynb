{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import functools # https://docs.python.org/3/library/functools.html\n",
    "import glob # short for global, is a function that's used to search for files that match a specific file pattern or name.\n",
    "import os\n",
    "import csv\n",
    "import SimpleITK as sitk # https://simpleitk.org/ (We can treat the format of the data files as a black box and use SimpleITK to load them into more familiar NumPy arrays.)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CandidateInfoTuple = namedtuple(\n",
    "    'CandidateInfoTuple',\n",
    "    'isNodule_bool, diameter_mm, series_uid, center_xyz',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getCandidateInfoList comments\n",
    "\n",
    "* *@functools.lru_cache(1) - in-memory cache decorator*\n",
    "* *requireOnDisk_bool=True - defaults to screening out series from data subsets that aren't in place yet.*\n",
    "* *We construct a set with all series_uids that are present on disk. This will let us use the data, even if we haven't downloaded all of the subsets yet.*\n",
    "* *For each of the candidate entries for a given series_uid, we loop through the annotations we collected earlier for the same series_uid and see if the two coordinates are\n",
    "close enough to consider them the same nodule.*\n",
    "* *If we don’t find diameter information for a nodule, that’s fine; we’ll just treat the nodule as having a 0.0 diameter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(1)\n",
    "def getCandidateInfoList(requireOnDisk_bool=True):\n",
    "    mhd_list = glob.glob('data/luna/subset*/*.mhd') # Return a possibly empty list of path names that match pathname\n",
    "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
    "\n",
    "    # First we need to group our annotations by series_uid, as that’s the first key we’ll use to cross-reference each row from the two files.\n",
    "    diameter_dict = {}\n",
    "\n",
    "    # seriesuid,coordX,coordY,coordZ,diameter_mm - header of 'annotations.csv'\n",
    "    with open('data/annotations.csv', 'r') as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "            annotationDiameter_mm = float(row[4])\n",
    "\n",
    "            diameter_dict.setdefault(series_uid, []).append(\n",
    "                (annotationCenter_xyz, annotationDiameter_mm)\n",
    "            )\n",
    "\n",
    "    # Now we’ll build our full list of candidates using the information in the candidates.csv file.\n",
    "    candidateInfo_list = []\n",
    "\n",
    "    # seriesuid,coordX,coordY,coordZ,class - header of 'candidates.csv'\n",
    "    with open('data/candidates.csv', 'r') as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "\n",
    "            # If a series_uid isn’t present, it’s in a subset we don’t have on disk, so we should skip it.\n",
    "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
    "                continue\n",
    "\n",
    "            isNodule = bool(int(row[4]))\n",
    "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "\n",
    "            candidateDiameter_mm = 0.0\n",
    "            for annotation_tuple in diameter_dict.get(series_uid, []):\n",
    "                annotationCenter_xyz, annotationDiameter_mm = annotation_tuple\n",
    "                for i in range(3):\n",
    "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
    "                    \n",
    "                    # Divides the diameter by 2 to get the radius, and divides the radius by 2 to require that the two nodule center points not be too far apart relative to the size of the nodule. \n",
    "                    # (This results in a bounding-box check, not a true distance check.) \n",
    "                    if delta_mm > annotationDiameter_mm / 4:\n",
    "                        break\n",
    "                    else:\n",
    "                        candidateDiameter_mm = annotationDiameter_mm\n",
    "                        break\n",
    "\n",
    "                candidateInfo_list.append(CandidateInfoTuple(\n",
    "                    isNodule,\n",
    "                    candidateDiameter_mm,\n",
    "                    series_uid,\n",
    "                    candidateCenter_xyz,\n",
    "                ))\n",
    "\n",
    "    # This means we have all of the actual nodule samples starting with the largest first, \n",
    "    # followed by all of the non-nodule samples (which don’t have nodule size information).\n",
    "    candidateInfo_list.sort(reverse=True)\n",
    "    return candidateInfo_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *The 10 subsets we discussed earlier have about 90 CT scans each (888 in total), with every CT scan represented as two files: one with a .mhd extension and one with a .raw extension. The data being split between multiple files is hidden behind the sitk routines, however, and is not something we need to be directly concerned with.*\n",
    "\n",
    "* *Continuing the __init__ method, we need to do a bit of cleanup on the ct_a val- ues. CT scan voxels are expressed in Hounsfield units (HU; https://en.wikipedia.org/wiki/Hounsfield_scale), which are odd units; air is –1,000 HU (close enough to 0 g/cc [grams per cubic centimeter] for our purposes), water is 0 HU (1 g/cc), and bone is at least +1,000 HU (2–3 g/cc).*\n",
    "\n",
    "* *It’s important to know that our data uses the range of –1,000 to +1,000.*\n",
    "\n",
    "* *Candidate center data is expressed in millimeters, not voxels. We need to transform our coordinates from the millimeter-based coordinate system (X,Y,Z) they’re expressed in, to the voxel-address-based coordinate system (I,R,C) used to take array slices from our CT scan data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![title](images/convertToIRC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with CT scans, we refer to the array dimensions as index, row, and column, because a separate meaning exists for X, Y, and Z, as illustrated in figure 10.6. The patient coordinate system defines positive X to be patient- left (left), positive Y to be patient-behind (posterior), and positive Z to be toward-patient- head (superior). **Left-posterior-superior is sometimes abbreviated LPS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![title](images/figure10_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The patient coordinate system is measured in millimeters and has an arbitrarily positioned origin that does not correspond to the origin of the CT voxel array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![title](images/figure10_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotted using square pixels, the non-cubic voxels can end up looking some- what distorted, similar to the distortion near the north and south poles when using a Mercator projection map. We will need to apply a scaling factor if we want the images to depict realistic proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CTs are commonly 512 rows by 512 columns, with the index dimension ranging from around 100 total slices up to perhaps 250 slices (250 slices times 2.5 millimeters is typically enough to contain the anatomical region of interest). This results in a lower bound of approximately 225 voxels, or about 32 million data points. Each CT specifies the voxel size in millimeters as part of the file metadata;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the math manually:\n",
    "* Flip the coordinates from IRC to CRI, to align with XYZ.\n",
    "* Scale the indices with the voxel sizes.\n",
    "* Matrix-multiply with the directions matrix, using @ in Python.\n",
    "* Add the offset for the origin.\n",
    "\n",
    "To go back from XYZ to IRC, we need to perform the inverse of each step in the reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CT:\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob(\n",
    "            'data/subset*/{}.mhd'.format(series_uid) # We don’t care to track which subset a given series_uid is in, so we wildcard the subset.\n",
    "        )[0]\n",
    "\n",
    "        # sitk.ReadImage implicitly consumes the .raw file in addition to the passed-in .mhd file.\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "\n",
    "        # Recreates an np.array since we want to convert the value type to np.float32\n",
    "        # ct_a is a three-dimensional array.\n",
    "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
    "\n",
    "        # Capping the values between -1000 and 1000\n",
    "        ct_a.clip(-1000, 1000, ct_a)\n",
    "\n",
    "        self.series_uid = series_uid\n",
    "        self.hu_a = ct_a\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
